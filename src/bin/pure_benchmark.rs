use std::sync::{Arc, Mutex};
use std::thread;
use std::time::{Duration, Instant};

fn main() {
    println!("Starting pure benchmark...");

    // ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    let transaction_count = 1000000; // 100ä¸‡ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³

    // ã‚·ãƒ³ã‚°ãƒ«ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
    println!("Running single-threaded benchmark...");
    let start_time = Instant::now();

    let mut successful = 0;
    for i in 0..transaction_count {
        if simulate_transaction(i) {
            successful += 1;
        }
    }

    let elapsed = start_time.elapsed();
    let tps = transaction_count as f64 / elapsed.as_secs_f64();

    println!(
        "Single-threaded benchmark completed in {:.2} seconds",
        elapsed.as_secs_f64()
    );
    println!(
        "Transactions: {} total, {} successful, {} failed",
        transaction_count,
        successful,
        transaction_count - successful
    );
    println!("Throughput: {:.2} TPS", tps);

    // ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
    println!("\nRunning multi-threaded benchmark...");

    // åˆ©ç”¨å¯èƒ½ãªCPUã‚³ã‚¢æ•°ã‚’å–å¾—
    let num_cpus = num_cpus::get();
    println!("Detected {} CPU cores", num_cpus);

    // ã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã§ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œ
    let thread_counts = vec![1, 2, 4, 8, 16, num_cpus];

    for &threads in thread_counts.iter().filter(|&&t| t <= num_cpus) {
        println!("Testing with {} threads...", threads);

        let start_time = Instant::now();
        let transactions_per_thread = transaction_count / threads;

        // ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’ç”Ÿæˆ
        let handles: Vec<_> = (0..threads)
            .map(|thread_id| {
                let start_idx = thread_id * transactions_per_thread;
                let end_idx = start_idx + transactions_per_thread;

                thread::spawn(move || {
                    let mut successful = 0;
                    for i in start_idx..end_idx {
                        if simulate_transaction(i) {
                            successful += 1;
                        }
                    }
                    successful
                })
            })
            .collect();

        // ã™ã¹ã¦ã®ã‚¹ãƒ¬ãƒƒãƒ‰ãŒå®Œäº†ã™ã‚‹ã®ã‚’å¾…ã¤
        let mut total_successful = 0;
        for handle in handles {
            total_successful += handle.join().unwrap();
        }

        let elapsed = start_time.elapsed();
        let tps = transaction_count as f64 / elapsed.as_secs_f64();

        println!("  Completed in {:.2} seconds", elapsed.as_secs_f64());
        println!("  Throughput: {:.2} TPS", tps);

        if tps >= 100000.0 {
            println!("  ğŸ‰ SUCCESS: Achieved 100K+ TPS with {} threads!", threads);
        }
    }

    // ä¸¦åˆ—ãƒãƒƒãƒå‡¦ç†ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
    println!("\nRunning parallel batch processing benchmark...");

    let batch_sizes = vec![100, 1000, 10000];

    for &batch_size in &batch_sizes {
        println!("Testing with batch size {}...", batch_size);

        let start_time = Instant::now();
        let batches = transaction_count / batch_size;
        let successful = Arc::new(Mutex::new(0));

        for batch_idx in 0..batches {
            let start_idx = batch_idx * batch_size;
            let end_idx = start_idx + batch_size;
            let successful_clone = Arc::clone(&successful);

            // ãƒãƒƒãƒå†…ã®ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä¸¦åˆ—å‡¦ç†
            let batch: Vec<_> = (start_idx..end_idx).collect();

            let batch_successful: usize = batch
                .into_par_iter()
                .filter(|&i| simulate_transaction(i))
                .count();

            let mut successful = successful_clone.lock().unwrap();
            *successful += batch_successful;
        }

        let elapsed = start_time.elapsed();
        let tps = transaction_count as f64 / elapsed.as_secs_f64();

        println!("  Completed in {:.2} seconds", elapsed.as_secs_f64());
        println!("  Throughput: {:.2} TPS", tps);

        if tps >= 100000.0 {
            println!(
                "  ğŸ‰ SUCCESS: Achieved 100K+ TPS with batch size {}!",
                batch_size
            );
        }
    }
}

// ã‚·ãƒ³ãƒ—ãƒ«ãªãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³å‡¦ç†ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
fn simulate_transaction(nonce: usize) -> bool {
    // ç½²åæ¤œè¨¼ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
    let signature_valid = verify_signature(nonce);

    // æ®‹é«˜ãƒã‚§ãƒƒã‚¯ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
    let balance_sufficient = check_balance(nonce);

    // æ‰‹æ•°æ–™ãƒã‚§ãƒƒã‚¯ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
    let fee_sufficient = check_fee(nonce);

    // ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
    if signature_valid && balance_sufficient && fee_sufficient {
        execute_transaction(nonce);
        true
    } else {
        false
    }
}

// ç½²åæ¤œè¨¼ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
fn verify_signature(nonce: usize) -> bool {
    // å®Ÿéš›ã®ç½²åæ¤œè¨¼ã®ä»£ã‚ã‚Šã«ã€ç°¡å˜ãªè¨ˆç®—ã‚’è¡Œã†
    let hash = (nonce * 13) % 100;
    hash > 5 // 95%ã®ç¢ºç‡ã§æˆåŠŸ
}

// æ®‹é«˜ãƒã‚§ãƒƒã‚¯ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
fn check_balance(nonce: usize) -> bool {
    // å®Ÿéš›ã®æ®‹é«˜ãƒã‚§ãƒƒã‚¯ã®ä»£ã‚ã‚Šã«ã€ç°¡å˜ãªè¨ˆç®—ã‚’è¡Œã†
    let balance = (nonce * 17) % 1000;
    let amount = (nonce * 7) % 900;
    balance >= amount
}

// æ‰‹æ•°æ–™ãƒã‚§ãƒƒã‚¯ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
fn check_fee(nonce: usize) -> bool {
    // å®Ÿéš›ã®æ‰‹æ•°æ–™ãƒã‚§ãƒƒã‚¯ã®ä»£ã‚ã‚Šã«ã€ç°¡å˜ãªè¨ˆç®—ã‚’è¡Œã†
    let fee = (nonce * 3) % 50;
    fee > 0
}

// ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
fn execute_transaction(nonce: usize) {
    // å®Ÿéš›ã®ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œã®ä»£ã‚ã‚Šã«ã€ç°¡å˜ãªè¨ˆç®—ã‚’è¡Œã†
    let _new_balance = (nonce * 17) % 1000 - (nonce * 7) % 900;
    let _new_recipient_balance = (nonce * 11) % 1000 + (nonce * 7) % 900;
}

// ä¸¦åˆ—ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ãŸã‚ã®ãƒˆãƒ¬ã‚¤ãƒˆ
trait IntoParallelIterator {
    type Item;
    type Iter: Iterator<Item = Self::Item>;

    fn into_par_iter(self) -> Self::Iter;
}

// Vecã«å¯¾ã™ã‚‹å®Ÿè£…
impl<T> IntoParallelIterator for Vec<T> {
    type Item = T;
    type Iter = std::vec::IntoIter<T>;

    fn into_par_iter(self) -> Self::Iter {
        self.into_iter()
    }
}
